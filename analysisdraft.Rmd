---
title: "Analysis Plan"
output: html_notebook
---
```{r}
library(lubridate)
library(dplyr)
```

#import finalized5
```{r}
finalized5<-finalized5
```

#create finalized 6 to account for missing goodreads data
#trim down finalized 5 to only include wikipedia data, imdb data, and financial data. We can leverage goodreads missing data into a dummy variable
#remove NAs using omit.na with this smaller selection of columns
#reindex data onto larger col subset

#import
```{r}
finalized6<-finalized5[,-c(1,24,25,26,27,28,29,30,13,10,7,19,20,21,22,23)]
length(finalized6)
finalized6cut<-na.omit(finalized6)

finalized6cut1 <- finalized6cut %>%
  mutate(
    # Check for duplicates in the 'movie_match' column.
    # The 'duplicated()' function marks the 2nd, 3rd, etc., instances as TRUE.
    # We use '|' (OR) with the reversed check to mark the 1st instance as TRUE, too.
    # This flags ALL rows that have a duplicated movie_match value.
    is_duplicated_movie_match = duplicated(movie_match) | duplicated(movie_match, fromLast = TRUE)
  )
df_no_duplicates_flag <- finalized6cut1 %>%
  # 1. Group by the movie_match key
  group_by(movie_match) %>%
  
  # 2. Within each group (i.e., for all duplicates), keep only the first row
  # The row is kept regardless of the is_duplicated_movie_match flag, 
  # as long as the movie_match value is the same.
  slice(1) %>%
  
  # 3. Always ungroup after grouping operations
  ungroup() %>%
  
  # 4. Optional: Remove the flag column if no longer needed
  select(-is_duplicated_movie_match)



finalized5cut1 <- finalized5 %>%
  mutate(
    # Check for duplicates in the 'movie_match' column.
    # The 'duplicated()' function marks the 2nd, 3rd, etc., instances as TRUE.
    # We use '|' (OR) with the reversed check to mark the 1st instance as TRUE, too.
    # This flags ALL rows that have a duplicated movie_match value.
    is_duplicated_movie_match = duplicated(movie_match) | duplicated(movie_match, fromLast = TRUE)
  )
df_no_duplicates_flag5 <- finalized5cut1 %>%
  # 1. Group by the movie_match key
  group_by(movie_match) %>%
  
  # 2. Within each group (i.e., for all duplicates), keep only the first row
  # The row is kept regardless of the is_duplicated_movie_match flag, 
  # as long as the movie_match value is the same.
  slice(1) %>%
  
  # 3. Always ungroup after grouping operations
  ungroup() %>%
  
  # 4. Optional: Remove the flag column if no longer needed
  select(-is_duplicated_movie_match)



#now unique movie matches
#length(unique(df_no_duplicates_flag$movie_match)) == df_no_duplicates_flag
finalized6<-left_join(df_no_duplicates_flag[,c(23)],df_no_duplicates_flag5,"movie_match")
```


#restate df
```{r}
df<-finalized6
```

#fix some variables numeric
```{r}
df$runtimeMinutes<-parse_number(df$runtimeMinutes)

df$worldwide_gross<-parse_number(df$worldwide_gross)

# Assuming your data frame is named 'df' and the column is 'release_date'
df <- df %>%
  mutate(
    # Step 1: Use a regular expression to find the first instance of four consecutive digits.
    # The pattern "\\d{4}" means "find 4 digits (0-9)"
    book_year_str = str_extract(Book, "\\d{4}"),
    
    # Step 2: Convert the extracted string to a numeric (integer) value.
    # This is essential for calculating the Book-to-Movie Lag later.
    book_year = as.numeric(book_year_str)
  )

```

#calculate other statistics
```{r}
df$book_movie_years<-df$wikiMovieYear-df$book_year

df$ROI<-(df$worldwide_gross-df$production_budget)/df$production_budget
```

#expand genres
```{r}
library(dplyr)
library(tidyr)

# Assuming your data frame is named 'df'

df <- df %>%
  # 1. Perform the separation and create the new genre columns
  separate(
    col = genres,          # The column to split (which is still retained by default)
    into = c("genre_1", "genre_2", "genre_3"), # Names of the new columns
    sep = ",",             # The delimiter
    extra = "drop",        # Drops any genres beyond the third one
    fill = "right",        # Fills missing values on the right with NA
    remove = FALSE         # ***CRITICAL: Ensures the original 'genres' column is KEPT***
  ) %>%
  
  # 2. Trim leading/trailing whitespace from the new columns for cleanliness
  mutate(
    genre_1 = trimws(genre_1),
    genre_2 = trimws(genre_2),
    genre_3 = trimws(genre_3)
  )

# The data frame 'df_processed' now contains:
# - The original 'genres' column (e.g., "Action, Adventure, Sci-Fi")
# - The new, clean columns: 'genre_1' ("Action"), 'genre_2' ("Adventure"), 'genre_3' ("Sci-Fi")
```


#eda
#genre
```{r}
#transpose on genre
df_genre <- df %>%
  pivot_longer(
    cols = starts_with("genre_"), # Stack genre_1, genre_2, genre_3
    names_to = "genre_position",
    values_to = "genre_name",
    values_drop_na = TRUE       # Remove the NA entries
  )

#EDA
ggplot(df_genre, aes(y = ROI, x = genre_name)) +
  geom_boxplot(fill = "skyblue", color = "darkblue", outlier.colour = "red", outlier.shape = 1) +
  labs(
    title = "ROI by Genre",
    x = "Genre",
    y = "ROI"
  ) +
  # Optional: Reorder the genres based on their median ROI for better comparison
  scale_y_discrete(limits = rev(sort(unique(df_genre$genre_name)))) +
  theme_minimal()
```

#book_movie_years
```{r}
#index out outliers
ggplot(df[(df[,30]<=400),],aes(x=book_movie_years,y=log(ROI))) + geom_point()
```

#production budget
```{r}
ggplot(df,aes(x=log(production_budget),y=log(ROI))) + geom_point()
head(df)
```

#runtime
```{r}
ggplot(df,aes(x=runtimeMinutes,y=log(ROI))) + geom_point()
```



#book_movie
```{r}
#remove beowulf
df_movie<-df[c(df[,38]!="Beowulf"),]
ggplot(df_movie,aes(x=book_year,y=log(ROI))) + geom_point()
```

#goodreads book popularity dummy three levels
```{r}
library(dplyr)

# Filter out rows where goodreads_numratings is NA
df_non_na <- df %>%
  filter(!is.na(goodreads_numratings))

# Calculate the tertile thresholds (33.3% and 66.7% percentiles) on the non-NA data
tertiles <- df_non_na %>%
  pull(goodreads_numratings) %>%
  quantile(probs = c(1/3, 2/3))

low_threshold <- unname(tertiles[1])
medium_threshold <- unname(tertiles[2])

print(paste("Low threshold (bottom 1/3) value:", low_threshold))
print(paste("Medium threshold (middle 1/3) value:", medium_threshold))
# 1. Ensure the popularity level is a factor with the correct order 
# (This is crucial for the x-axis)
library(ggplot2)

df_processed <- df_non_na %>%
  # 1. Create the 3-level popularity column
  mutate(
    book_popularity_level = case_when(
      goodreads_numratings <= low_threshold ~ "Low",
      goodreads_numratings > low_threshold & goodreads_numratings <= medium_threshold ~ "Medium",
      goodreads_numratings > medium_threshold ~ "High",
      TRUE ~ "Other" # Should catch any remaining odd cases
    ),
    
    # 2. Convert to a factor for proper plotting/ordering
    book_popularity_level = factor(
      book_popularity_level,
      levels = c("Low", "Medium", "High")
    ),
    
    # 3. Calculate ROI (re-running this to be safe, if not already done)
    # Ensure financial columns are numeric
    worldwide_gross = as.numeric(worldwide_gross),
    production_budget = as.numeric(production_budget),
    ROI = (worldwide_gross - production_budget) / production_budget
  ) %>%
  # 4. Filter out any remaining NAs in ROI before plotting
  drop_na(ROI)


# 5. Generate the Box Plot
ggplot(df_processed, aes(x = book_popularity_level, y = log(ROI))) +
  geom_violin(fill = "salmon", color = "darkred", outlier.colour = "red", outlier.shape = 1) +
  labs(
    title = "Movie ROI Distribution by Book Popularity Level",
    subtitle = "Book Popularity based on Goodreads Number of Ratings (Tertiles)",
    x = "Book Popularity Level",
    y = "Return on Investment (ROI)"
  ) +
  theme_minimal()
```

```{r}
head(df)
df
```





#write csv
```{r}
#write_csv(df,"/Users/patrickdennen/Desktop/capstone/finalized6.csv")
head(tibble(FINALDF))
FINAL_DF<-FINALDF[,c(3,4,6,11,12,13,14,15,16,17,18,19,22,24,25,26,27,28,29,30,31,32,33,34,37,39,40,41,44,45,46,47,48)]
write_csv(FINAL_DF,"/Users/patrickdennen/Desktop/capstone/FINAL_DF.csv")

df<-read.csv("/Users/patrickdennen/Desktop/capstone/FINAL_DF.csv")
```








#model

```{r}
dfmodel<-df[-c(210),c(6,26,31,32,30,8,33)]
dfmodel$bmy_12<-(dfmodel$book_movie_years >= median(dfmodel$book_movie_years))
dfmodel<-dfmodel[,-c(5)]
```

#encode categorical variables
```{r}
# Create a copy of the sample data for encoding
sample_dfmodel1 <- dfmodel

# Use model.matrix for each categorical variable
genre1_dummies <- model.matrix(~ genre_1 - 1, data = dfmodel)
name_match_dummies <- model.matrix(~ name_match - 1, data = dfmodel)
bmy_dummies <- model.matrix(~ bmy_12 - 1, data = dfmodel)
cs_dummies <- model.matrix(~ contains_series - 1, data = dfmodel)
#contains_series_dummies <- model.matrix(~ contains_series - 1, data = dfmodel)

# Combine encoded columns with the remaining dataset
sample_dfmodel1 <- cbind(
  dfmodel,
  genre1_dummies,
  name_match_dummies,
  bmy_dummies
)

# View the structure of the encoded dataset

sample_dfmodel<-sample_dfmodel1[,c(1,2,3,13,17,19,21)]
str(sample_dfmodel)


```

#random sample
```{r}
# 1. Set a "seed" for reproducibility
# This ensures you get the exact same random split every time you run the code.
set.seed(123) 

# 2. define your split ratio (e.g., 0.7 for 70% training data)
split_ratio <- 0.7

# 3. Create a list of random indices
# We take a sample of row numbers equal to 70% of the total rows
train_index <- sample(1:nrow(sample_dfmodel), split_ratio * nrow(sample_dfmodel))

# 4. Create the two new dataframes
train_data <- sample_dfmodel[train_index, ]
test_data  <- sample_dfmodel[-train_index, ]

# 5. Verify the split (Optional)
print(paste("Training rows:", nrow(train_data)))
print(paste("Testing rows:", nrow(test_data)))
```


#install packages
```{r}
install.packages("rpart")
install.packages("rpart.plot")
```

```{r}
library(rpart)
library(rpart.plot)

# 1. Train the Regression Tree
# method = "anova" tells R this is for a continuous target (Regression)
# If you leave it out, R usually figures it out, but it's safer to be explicit.
tree_model <- rpart(ROI ~ ., 
                    data = train_data, 
                    method = "anova")

# 2. Visualize the Tree (The best part!)
# type = 3 draws separate split labels for left and right
# digits = 3 ensures you see enough precision in the numbers
rpart.plot(tree_model, type = 1, digits = 3, fallen.leaves = TRUE)

# 3. Make Predictions
dt_predictions <- predict(tree_model, newdata = test_data)

# 4. Check Performance (RMSE)
rmse_dt <- sqrt(mean((test_data$Target_Variable - dt_predictions)^2))
print(paste("Decision Tree RMSE:", rmse_dt))
```



#train decision tree
```{r}
train_decision_tree <- function(data, target_col, tune_grid) {
  # Ensure target column is a factor
  data[[target_col]] <- factor(data[[target_col]], levels = c(0, 1))
  
  # Split data into training and testing sets
  set.seed(123)
  train_indices <- createDataPartition(data[[target_col]], p = 0.8, list = FALSE)
  train_data <- data[train_indices, ]
  test_data <- data[-train_indices, ]
  
  # Train control with cross-validation
  train_control <- trainControl(
    method = "cv",           # Cross-validation
    number = 5,              # 5-fold CV
    classProbs = TRUE,       # Enable probabilities
    summaryFunction = twoClassSummary # For ROC evaluation
  )

    # Train the model with parameter tuning
  model <- train(
    as.formula(paste(target_col, "~ .")), 
    data = train_data, 
    method = "rpart", 
    trControl = train_control, 
    tuneGrid = tune_grid,   # Grid for tuning
    metric = "ROC"          # Use ROC to select the best model
  )
  
  # Predict on test data
  predictions_prob <- predict(model, newdata = test_data, type = "prob")
  predictions_class <- predict(model, newdata = test_data)
  
  # Calculate metrics
  confusion <- confusionMatrix(predictions_class, test_data[[target_col]])
  roc_curve <- roc(test_data[[target_col]], predictions_prob[, 2])
  metrics <- list(
    Accuracy = confusion$overall["Accuracy"],
    Precision = confusion$byClass["Pos Pred Value"],
    Recall = confusion$byClass["Sensitivity"],
    F1_Score = 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) / 
                (confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"]),
    ROC_AUC = auc(roc_curve)
  )
  
  # Return model, metrics, and the confusion matrix
  return(list(
    Model = model,
    Metrics = metrics,
    Confusion_Matrix = confusion
  ))
}

```

#gradient boost and surrogate tree
```{r}
library(gbm)

set.seed(123)

# 1. Train the model with Cross-Validation
# Note: We use 'sample_dfmodel' (your full data), not 'train_data'
cv_boost_model <- gbm(ROI ~ ., 
                      data = sample_dfmodel,
                      distribution = "gaussian",
                      n.trees = 1000,
                      interaction.depth = 4,
                      shrinkage = 0.01,
                      n.minobsinnode = 10,
                      bag.fraction = 0.5,      # <--- IMPORTANT for small data
                      cv.folds = 5)            # <--- This turns on 5-Fold CV

# 2. Check the "Optimal" number of trees based on CV error
# This helps prevent overfitting, which is common with small data
best_iter <- gbm.perf(cv_boost_model, method = "cv")
print(paste("Optimal Trees:", best_iter))

# 3. Get the Cross-Validation RMSE
# This number is much more trustworthy than your single-split RMSE
cv_rmse <- sqrt(min(cv_boost_model$cv.error))
print(paste("Cross-Validated RMSE:", cv_rmse))

library(rpart)
library(rpart.plot)



# 1. Generate predictions from your complex Boosted Model
# n.trees = best_iter (the optimal number you found in the previous step)
# If you didn't save best_iter, just use 1000 or however many you trained.
gbm_predictions <- predict(cv_boost_model, 
                           newdata = sample_dfmodel, 
                           n.trees = best_iter)

# 2. Create a temporary dataframe
# We want to predict the "GBM_Score", not the real "ROI"
surrogate_data <- sample_dfmodel
surrogate_data$GBM_Predictions <- gbm_predictions

# Remove the actual ROI so the tree doesn't cheat (optional, but good practice)
surrogate_data$ROI <- NULL 

# 3. Train a single "Surrogate" Tree
# We set cp (complexity parameter) slightly loose to ensure we get a readable tree
surrogate_tree <- rpart(GBM_Predictions ~ ., 
                        data = surrogate_data, 
                        method = "anova",
                        control = rpart.control(cp = 0.005))

# 4. Visualize the "Logic" of your Boosted Model
rpart.plot(surrogate_tree, 
           type = 3, 
           digits = 3, 
           fallen.leaves = TRUE,
           main = "Surrogate Tree: Approximate Logic of GBM Model")
```

